{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrendanCreates/Finetuning-Mistral-7B-Legal/blob/Brendan's-Parallel-Branch/ese577_fall25_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BEFORE YOU START, CHANGE THE RUNTIME TO REQUEST A T4 GPU"
      ],
      "metadata": {
        "id": "_dG10yOUmr0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEuuB_GwZe-4"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "# You will need at a minimum the following packages. Feel free to install\n",
        "# additional ones as needed\n",
        "!pip install google-generativeai\n",
        "!pip install datasets\n",
        "!pip install -U bitsandbytes\n",
        "!pip install transformers\n",
        "!pip install -U peft\n",
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!pip install -U trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, \\\n",
        "    BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
        "import torch\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "HlgOeanvFUlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this block, I include code for parsing the Intro chapter from a text file.\n",
        "# I ran the following two lines on a linux terminal. You can find the equi-\n",
        "# valent for your OS or find an online tool for converting PDF into text.\n",
        "# >>> pdftotext -nopgbrk MIT_6390_chapter_Introduction.pdf\n",
        "# >>> sed -r ':a /[a-zA-Z,\\ ]$/N;s/(.)\\n/\\1 /;ta' \\\n",
        "#        MIT_6390_chapter_Introduction.txt > \\\n",
        "#        MIT_6390_chapter_Introduction_reformat.txt\n",
        "#\n",
        "# Once the PDF was converted to a text file, I manually looked through it to:\n",
        "# - Remove ninformative lines (e.g., \"Last updated: ...\", \"MIT 6390\", ...)\n",
        "# - Remove comments, which come somewhat poorly organized\n",
        "# - Remove double line breaks\n",
        "# - Fix up equations a bit so that they made sense in text format\n",
        "#\n",
        "# I did all this in a simple text editor (I used Sublime Text). Then I simply\n",
        "# uploaded the file to Colab and ran the following code to split the text into\n",
        "# informative paragraphs. This required a bit of iterating back and forth to\n",
        "# make sure that no paragraph was \"trailing\" from the previous one.\n",
        "\n",
        "import glob\n",
        "\n",
        "for file in glob.glob(\"data/source/MIT_6390_chapter_*.txt\"):\n",
        "  with open(file) as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  paragraphs = []\n",
        "  min_chars = 200\n",
        "  par = lines[0]\n",
        "  for ln in lines[1:]:\n",
        "    if ln[0] == \"â€¢\" or ln[0].isdigit() and ln[1:3] == \". \":\n",
        "      # Part of a list, combine with previous items\n",
        "      par += ln\n",
        "    else:\n",
        "      paragraphs.append(par.strip())  # Remove trailing whitespace and store\n",
        "      par = ln               # Start new paragraph\n",
        "\n",
        "  paragraphs.append(par)\n",
        "\n",
        "for line in paragraphs:\n",
        "  print(line)\n",
        "  print('---')\n",
        "print(len(paragraphs))\n",
        "\n",
        "# I have uploaded the reformatted text file I used to generate data for the\n",
        "# Intro chapter. You should follow a similar process to create data from any\n",
        "# source you wish to use. Note: the better you clean up your data, the more\n",
        "# useful your final model will be."
      ],
      "metadata": {
        "id": "VgO4IDafFWdi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block contains the code to interact with the Google Gemini 2.5 Flash API\n",
        "# to request questions and answers for your data. It loops through a list of\n",
        "# paragraphs and requests Gemini to create one question for each paragraph indi-\n",
        "# vidually. Your main job here is to write appropriate prompts that lead Gemini\n",
        "# to generate useful questions. You can also consider generating more/fewer\n",
        "# questions per paragraph, or merging paragraphs if you think that will help.\n",
        "# Be sure to document any changes you make in your report!\n",
        "\n",
        "# TODO: create your own Gemini API key, and either paste it here (and then\n",
        "# remove it before turning in your report) or save it in a file and load it here.\n",
        "geminiApiKey= ...\n",
        "genai.configure(api_key=geminiApiKey)\n",
        "cfg = genai.types.GenerationConfig(max_output_tokens=4000)\n",
        "sys_msg_train = (\n",
        "'''\n",
        "\n",
        "  TODO: write your own system prompt that encourages Gemini to generate data\n",
        "  in the format you want it. Giving examples to Gemini is often useful.\n",
        "\n",
        "'''\n",
        ")\n",
        "print(sys_msg_train)\n",
        "print()\n",
        "model_train = genai.GenerativeModel('gemini-2.5-flash', system_instruction=sys_msg_train)\n",
        "qa_pairs_train = []\n",
        "for par in paragraphs[:5]:\n",
        "  qa_pairs_train.append(model_train.generate_content(par, generation_config=cfg).text)\n",
        "  print(qa_pairs_train[-1])"
      ],
      "metadata": {
        "id": "REHhIsdxFlCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg_val = (\n",
        "\n",
        "'''\n",
        "\n",
        "  TODO: write your own system prompt that encourages Gemini to generate data\n",
        "  in the format you want it for val. Giving examples to Gemini is often useful.\n",
        "\n",
        "'''\n",
        "\n",
        ")\n",
        "print(sys_msg_val)\n",
        "print()\n",
        "model_val = genai.GenerativeModel('gemini-2.5-flash', system_instruction=sys_msg_val)\n",
        "qa_pairs_val = []\n",
        "for par in paragraphs[:5]:\n",
        "  qa_pairs_val.append(model_val.generate_content(par, generation_config=cfg).text)\n",
        "  print(qa_pairs_val[-1])"
      ],
      "metadata": {
        "id": "DnZzislTcU-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create HuggingFace datasets\n",
        "dataset_train = Dataset.from_pandas(pd.DataFrame(qa_pairs_train, columns=[\"text\"]))\n",
        "dataset_val = Dataset.from_pandas(pd.DataFrame(qa_pairs_val, columns=[\"text\"]))\n",
        "dataset = DatasetDict({\"train\": dataset_train, \"test\": dataset_val})\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "Cimifn0iFtvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token <TODO: CREATE YOUR OWN TOKEN>"
      ],
      "metadata": {
        "id": "tDEZ9CWujjT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model -- Skeleton\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    '''\n",
        "      TODO: find an appropriate configuration to use here for quantization\n",
        "    '''\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    '''\n",
        "      TODO: find an appropriate configuration to use here for finetuning\n",
        "    '''\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "Th-41Hfbge0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.bos_token, tokenizer.eos_token"
      ],
      "metadata": {
        "id": "4qa_PxyjfNkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA config -- Skeleton\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    '''\n",
        "      TODO: find an appropriate configuration to use here for LoRA\n",
        "    '''\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "CKL2WHVNGaBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters -- Skeleton\n",
        "training_arguments = TrainingArguments(\n",
        "    '''\n",
        "      TODO: find an appropriate configuration to use here for training\n",
        "    '''\n",
        ")"
      ],
      "metadata": {
        "id": "azVm7Pv9Gmrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=None,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0NHWlXq7GonK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "trainer.model.save_pretrained(\"ESE577_chatbot\")\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "WST5JLtCIOdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model locally\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, truncation=True)\n",
        "def build_prompt(question):\n",
        "  prompt = f\"<s>[INST]@ESE577. {question}. [/INST]\"\n",
        "  return prompt\n",
        "\n",
        "while True:\n",
        "  question = input(\"Enter your ESE577-related question (hit Enter to exit): \").strip()\n",
        "  if not question:\n",
        "    break\n",
        "  prompt = build_prompt(question)\n",
        "  answer = pipe(prompt)\n",
        "  print(answer[0][\"generated_text\"])\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "ayRYSNcuI_sZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}